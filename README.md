# WebsiteCrawler

Набор скриптов предназначен для сбора текстовой информации с html страниц сайта, находящейся внутри определенного классового селектора. Скрипт был протестирован для [сайта Донецкого государственного унивеситета](https://donnu.ru/), в данном случае классовым селектором выступал `content clearfix`:

![image](https://github.com/user-attachments/assets/f3a82e0e-916d-4987-ad84-12596c7f6a65)

Также скрипт собирает все файлы, на которые есть ссылки на целевом сайте, необходимость этого сбора регулируется специальным значением командной строки.

## Запуск

Перед работой со скриптами необходимо установить необходимые зависимости:

```bash
  pip install -r requirements.txt 
```

Для запуска сборщика информации необходимо выполнить команду

```bash
  python main.py <args> 
```
со следующими аргументами:

- -u/--url: корневая ссылка, с которой начинается обход всего сайта. **Обязательный параметр**;
- -tag/--content_class: классовый селектор, обозначающий текстовый блок, откуда должна собираться информация из html страниц. **Обязательный параметр**;
- -d/--depth: глубина обхода, числовое значение, показывающее на сколько страниц вглубь необходимо собирать данные. **Необязательный параметр**, по умолчанию `None`, что говорит о необходимости полного обхода сайта (может занять достаточно много времени);
- -n/--normalize_filenames: флаг, сигнализирующий о том, что необходима нормализация имен текстовых файлов, полученных из html страниц. Нормализация заключается в поиске наибольшей общей части в именах полученных файлов и ее удалении. Имена файлов берутся из `title` страниц, процедура полезна, если они имеют какую-либо копирайт часть. **Необязательный параметр**, если не указан - нормализация имен файлов производиться не будет;
- -ignore_files/--ignore_files: флаг, сигнализирующий о необходимости загрузки только текстового контента страниц, игнорируя при этом различные файлы. **Необязательный параметр**, если не указан - помимо html страниц скрипт загрузит файлы, на котороые найдет ссылки с учетом глубины обхода.

## Объединитель текстовых файлов

Главной целью набора скриптов является сбор данных с сайта (в частности с сайта образовательной организации) для дообучения на них большой языковой модели (LLM) с целью создания интеллектуального ассистента в виде чат-бота. Для дообучения, в некоторых случаях, может понадобиться чтобы весь текстовый корпус находился в одном файле. Для этого был создан еще один скрипт - `text_files_merger.py`, позволяющий объединить все текстовые файлы из целевой папки в указанный текстовый файл.

Для запуска сборщика информации необходимо выполнить команду

```bash
  python text_files_merger.py <args> 
```
с двумя аргументами - путем к папке с текстовыми файлами и путем к выходному файлу. Пример вызова команды:

```bash
  python text_files_merger.py data/crawl_at_01.01.2024 output.txt 
```
